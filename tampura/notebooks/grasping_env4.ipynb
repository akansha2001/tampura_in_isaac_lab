{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tampura.environment import TampuraEnv\n",
    "from tampura.spec import ProblemSpec\n",
    "from tampura.structs import (\n",
    "    AbstractBelief,\n",
    "    ActionSchema,\n",
    "    StreamSchema,\n",
    "    AliasStore,\n",
    "    Belief,\n",
    "    NoOp,\n",
    "    Predicate,\n",
    "    State,\n",
    "    effect_from_execute_fn,\n",
    "    Observation\n",
    ")\n",
    "import logging \n",
    "from tampura.symbolic import OBJ, Atom, ForAll\n",
    "from tampura.policies.tampura_policy import TampuraPolicy\n",
    "from tampura.config.config import get_default_config, setup_logger\n",
    "\n",
    "PICK_SUCCESS = 0.9\n",
    "PLACE_SUCCESS = 0.8\n",
    "OBJECTS = [f\"{OBJ}o1\"]\n",
    "LOCATIONS = [\"init\"]\n",
    "GOAL_LOCATIONS = [\"goal_loc_o_o1\"]\n",
    "ROBOT_LOCATION = \"loc_rob\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State of the environment\n",
    "class PickAndPlace(State):\n",
    "    store: AliasStore = AliasStore()\n",
    "    \n",
    "# Observation space\n",
    "@dataclass\n",
    "class LocationObservation(Observation):\n",
    "    at: Dict[str,str] = field(default_factory=lambda: {})\n",
    "\n",
    "# Belief space\n",
    "class LocationBelief(Belief):\n",
    "    def __init__(self, at={}):\n",
    "        self.at = at\n",
    "\n",
    "    def update(self, a, o, s):\n",
    "        return LocationBelief(at=o.at)\n",
    "\n",
    "    def abstract(self, store: AliasStore):\n",
    "        return AbstractBelief([Atom(\"at\", [o,loc]) for o,loc in self.at.items()])\n",
    "\n",
    "    def vectorize(self):\n",
    "        pass\n",
    "    \n",
    "# Sample function for stream schema\n",
    "    \n",
    "def locate_sample_fn(input_sym, store):\n",
    "    \n",
    "    loc_obj = store.get(\"loc_\"+input_sym[0])\n",
    "    \n",
    "    return loc_obj\n",
    "        \n",
    "\n",
    "# Action simulators\n",
    "def pick_execute_fn(a, b, s, store):\n",
    "    \n",
    "    if random.random() < PICK_SUCCESS:\n",
    "        loc_rob = store.get(\"loc_rob\")\n",
    "        b.at[a.args[0]] = loc_rob\n",
    "        store.set(\"loc_\"+a.args[0],loc_rob,\"location\")\n",
    "\n",
    "    \n",
    "    return store, LocationObservation(b.at)\n",
    "\n",
    "def place_execute_fn(a, b, s, store):\n",
    "    \n",
    "    if random.random() < PLACE_SUCCESS:\n",
    "        place_loc = store.get(a.args[1])\n",
    "        b.at[a.args[0]] = place_loc\n",
    "        store.set(\"loc_\"+a.args[0],place_loc,\"location\")\n",
    "\n",
    "    \n",
    "    return store, LocationObservation(b.at)\n",
    "\n",
    "\n",
    "\n",
    "# Set up environment dynamics\n",
    "class ToyDiscrete(TampuraEnv):\n",
    "    def initialize(self):\n",
    "        store = AliasStore()\n",
    "        store.set(\"loc_rob\", ROBOT_LOCATION, \"location\")\n",
    "        for (o,loc,goal_loc) in zip(OBJECTS,LOCATIONS,GOAL_LOCATIONS):\n",
    "            store.set(o, o, \"physical\")\n",
    "            store.set(\"loc_\"+o, loc, \"location\")\n",
    "            store.set(\"goal_loc_\"+o, goal_loc, \"location\")\n",
    "\n",
    "\n",
    "        return LocationBelief(), store\n",
    "\n",
    "    def get_problem_spec(self) -> ProblemSpec:\n",
    "        predicates = [\n",
    "            Predicate(\"located\", [\"physical\"]),\n",
    "            Predicate(\"at\", [\"physical\",\"location\"])\n",
    "        ]\n",
    "        \n",
    "        stream_schemas = [\n",
    "            StreamSchema(\n",
    "                name=\"locate\",\n",
    "                inputs=[\"?o1\"],\n",
    "                input_types=[\"physical\"],\n",
    "                output=\"?loc_o1\",\n",
    "                output_type=\"location\",\n",
    "                certified=[Atom(\"located\",[\"?o1\"])],\n",
    "                sample_fn=locate_sample_fn                \n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        action_schemas = [\n",
    "            ActionSchema(\n",
    "                name=\"pick\",\n",
    "                inputs=[\"?o1\"],\n",
    "                input_types=[\"physical\"],\n",
    "                preconditions=[Atom(\"located\",[\"?o1\"])],\n",
    "                verify_effects=[Atom(\"at\", [\"?o1\",\"loc_rob\"])],\n",
    "                execute_fn=pick_execute_fn,\n",
    "                effects_fn=effect_from_execute_fn(pick_execute_fn),\n",
    "            ),\n",
    "            ActionSchema(\n",
    "                name=\"place\",\n",
    "                inputs=[\"?o1\",\"?loc_o1\"],\n",
    "                input_types=[\"physical\",\"location\"],\n",
    "                preconditions=[Atom(\"at\",[\"?o1\",\"loc_rob\"])],\n",
    "                verify_effects=[Atom(\"at\", [\"?o1\",\"?loc_o1\"])],\n",
    "                execute_fn=place_execute_fn,\n",
    "                effects_fn=effect_from_execute_fn(place_execute_fn),\n",
    "            ),\n",
    "            NoOp(),\n",
    "        ]\n",
    "\n",
    "        # reward = Atom(\"at\",[\"o_o1\",\"loc_rob\"]) # WORKS??\n",
    "        reward = Atom(\"at\", [\"o_o1\",\"goal_loc_o_o1\"])\n",
    "\n",
    "        spec = ProblemSpec(\n",
    "            predicates=predicates,\n",
    "            stream_schemas=stream_schemas,\n",
    "            action_schemas=action_schemas,\n",
    "            reward=reward,\n",
    "        )\n",
    "\n",
    "        return spec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create environment and planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planner\n",
    "cfg = get_default_config(save_dir=os.getcwd())\n",
    "\n",
    "# Set some print options to print out abstract belief, action, observation, and reward\n",
    "cfg[\"print_options\"] = \"ab,a,o,r\"\n",
    "cfg[\"vis_graph\"] = True\n",
    "cfg[\"batch_size\"] = 100\n",
    "cfg[\"num_samples\"] = 1000\n",
    "\n",
    "# Initialize environment\n",
    "env = ToyDiscrete(config=cfg)\n",
    "b0, store = env.initialize()\n",
    "\n",
    "# Set up logger to print info\n",
    "setup_logger(cfg[\"save_dir\"], logging.INFO)\n",
    "\n",
    "# Initialize the policy\n",
    "planner = TampuraPolicy(config = cfg, problem_spec = env.problem_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Planner\n",
    "Make sure symk is installed (see README) before running the Tampura planner.\n",
    "With the default settings, the planner should pick both every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========t=0==========\n",
      "Abstract Belief: AbstractBelief(items=[])\n",
      "Reward: 0.0\n",
      "Flat Stream Sampling\n",
      "Sampling StreamSchema(name='locate', inputs=['?o1'], input_types=['physical'], output='?loc_o1', output_type='location', preconditions=[], certified=[Atom(pred_name='located', args=['?o1'])], sample_fn=<function locate_sample_fn at 0x7fd4def93ac0>)(['o_o1'])\n",
      "Progressive widening on action place(o_o1), 3.0>0\n",
      "Progressive widening on action place(o_o1), 3.4460950649911055>1\n",
      "Progressive widening on action place(o_o1), 3.737192818846552>2\n",
      "Progressive widening on action place(o_o1), 3.9585237323186826>3\n",
      "Progressive widening on action place(o_o1), 4.139188984383645>4\n",
      "Progressive widening on action place(o_o1), 5.010832957004431>5\n",
      "Progressive widening on action place(o_o1), 6.0>6\n",
      "Progressive widening on action place(o_o1), 7.016828512284062>7\n",
      "Progressive widening on action place(o_o1), 8.0018058251898>8\n",
      "Action: pick(o_o1)\n",
      "Observation: LocationObservation(at={'o_o1': 'loc_rob'})\n",
      "\n",
      "==========t=1==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='at', args=['o_o1', 'loc_rob'])])\n",
      "Reward: 0.0\n",
      "Progressive widening on action place(o_o1), 9.0>9\n",
      "Action: place(o_o1, goal_loc_o_o1)\n",
      "Observation: LocationObservation(at={'o_o1': 'goal_loc_o_o1'})\n",
      "\n",
      "==========t=2==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='at', args=['o_o1', 'goal_loc_o_o1'])])\n",
      "Reward: 1.0\n",
      "Action: no-op()\n",
      "Observation: None\n",
      "\n",
      "==========t=3==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='at', args=['o_o1', 'goal_loc_o_o1'])])\n",
      "Reward: 1.0\n",
      "Action: no-op()\n",
      "Observation: None\n",
      "\n",
      "==========t=4==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='at', args=['o_o1', 'goal_loc_o_o1'])])\n",
      "Reward: 1.0\n",
      "Action: no-op()\n",
      "Observation: None\n",
      "\n",
      "==========t=5==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='at', args=['o_o1', 'goal_loc_o_o1'])])\n",
      "Reward: 1.0\n",
      "Action: no-op()\n",
      "Observation: None\n",
      "\n",
      "==========t=6==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='at', args=['o_o1', 'goal_loc_o_o1'])])\n",
      "Reward: 1.0\n",
      "Action: no-op()\n",
      "Observation: None\n",
      "\n",
      "==========t=7==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='at', args=['o_o1', 'goal_loc_o_o1'])])\n",
      "Reward: 1.0\n",
      "Action: no-op()\n",
      "Observation: None\n",
      "\n",
      "==========t=8==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='at', args=['o_o1', 'goal_loc_o_o1'])])\n",
      "Reward: 1.0\n",
      "Action: no-op()\n",
      "Observation: None\n",
      "\n",
      "==========t=9==========\n",
      "Abstract Belief: AbstractBelief(items=[Atom(pred_name='at', args=['o_o1', 'goal_loc_o_o1'])])\n",
      "Reward: 1.0\n",
      "Action: no-op()\n",
      "Observation: None\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "history,store = planner.rollout(env, b0, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Atom(pred_name='located', args=['o_o1'])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.certified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
